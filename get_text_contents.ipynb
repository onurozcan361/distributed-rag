{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits a block of text into sentences using a simple regex.\n",
    "    For more robust splitting, consider using nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sentences if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    \"\"\"\n",
    "    Removes characters that are invalid in file names.\n",
    "    \"\"\"\n",
    "    # Remove or replace characters that are invalid in filenames on most OSs.\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_text_lines(page):\n",
    "    \"\"\"\n",
    "    Extracts meaningful content from a wikipediaapi Page object.\n",
    "    - The page title is added as the first line.\n",
    "    - The summary (lead text) is split into sentences (one per line).\n",
    "    - For each section and its subsections (except ignored ones),\n",
    "      adds the section title and text (split into sentences).\n",
    "    Returns a list of lines.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    # Add page title.\n",
    "    lines.append(page.title)\n",
    "    \n",
    "    # Add summary.\n",
    "    if page.summary:\n",
    "        lines.extend(split_into_sentences(page.summary))\n",
    "    \n",
    "    # Define section titles to ignore (case-insensitive).\n",
    "    ignore_sections = {\"see also\", \"references\", \"further reading\", \"external links\", \"notes\", \"citations\", \"bibliography\"}\n",
    "    \n",
    "    def process_section(section):\n",
    "        # Skip if the section title is one of the ignored sections.\n",
    "        if section.title.strip().lower() in ignore_sections:\n",
    "            return\n",
    "        # Add the section title if available.\n",
    "        if section.title:\n",
    "            lines.append(section.title)\n",
    "        # Add the section text.\n",
    "        if section.text:\n",
    "            lines.extend(split_into_sentences(section.text))\n",
    "        # Process any subsections recursively.\n",
    "        for subsec in section.sections:\n",
    "            process_section(subsec)\n",
    "    \n",
    "    for sec in page.sections:\n",
    "        process_section(sec)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Change these variables as needed.\n",
    "    topic = \"Law\"  # Topic name (for JSON lookup)\n",
    "    json_filename = \"Article JSONs/Law_articles.json\"  # JSON file produced earlier.\n",
    "    \n",
    "    # Create a folder for the topic. Remove spaces to form a valid folder name.\n",
    "    folder_name = topic.replace(\" \", \"\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    # Create a wikipediaapi object with a proper user-agent.\n",
    "    wiki = wikipediaapi.Wikipedia(language='en', user_agent=\"UniCourseWikipediaBot (mehmetaltintas@etu.edu.tr)\")\n",
    "    \n",
    "    # Load the JSON file with the articles.\n",
    "    with open(json_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    print(f\"Found {len(articles)} articles in '{json_filename}'.\")\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.get(\"title\")\n",
    "        print(f\"Processing page: {title}\")\n",
    "        \n",
    "        page = wiki.page(title)\n",
    "        if not page.exists():\n",
    "            print(f\"Page '{title}' does not exist or could not be fetched.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract content lines from the page.\n",
    "        content_lines = get_page_text_lines(page)\n",
    "        content = \"\\n\".join(content_lines)\n",
    "        \n",
    "        # Create a safe filename for the article.\n",
    "        safe_title = clean_filename(title)\n",
    "        file_path = os.path.join(folder_name, f\"{safe_title}.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                f_out.write(content)\n",
    "            print(f\"Saved content to '{file_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing file '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
